#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import json
import time
import uuid
import argparse
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import torch.nn.functional as F
from tqdm import tqdm

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--prompt_fp', type=str, required=True,
        help='Path to the prompt template file containing {{Document}} and {{Summary}} placeholders'
    )
    parser.add_argument(
        '--summeval_fp', type=str, required=True,
        help='Path to the SumEval dataset JSON file'
    )
    parser.add_argument(
        '--save_fp', type=str, required=True,
        help='Path where the final scored results will be saved'
    )
    parser.add_argument(
        '--max_new_tokens', type=int, default=1000,
        help='Maximum number of tokens to generate'
    )
    parser.add_argument(
        '--top_k', type=int, default=10,
        help='Number of top token logprobs to record at each generation step'
    )
    parser.add_argument(
        '--model', type=str, default="Qwen/Qwen2.5-72B-Instruct",
        # Or "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
        help='Model used to evaluate as a judge'
    )
    parser.add_argument(
        '--temperature', type=float, default=1.0,
        help='Sampling temperature. Use 1.0 for GPT-4o mini style, 0 for DeepSeek/Qwen local (greedy)'
    )
    parser.add_argument(
        '--limit', type=int, default=None,
        help='If set, only process the first N samples (useful for quick tests)'
    )

    args = parser.parse_args()

    # Load dataset and prompt template
    with open(args.summeval_fp, 'r', encoding='utf-8') as f:
        data = json.load(f)
    prompt_template = open(args.prompt_fp, 'r', encoding='utf-8').read()

    # Initialize model and tokenizer
    model_name = args.model
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True,
        cache_dir=os.environ.get("HF_HOME", None),
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        trust_remote_code=True,
        torch_dtype=torch.float16,
        device_map="auto",
        cache_dir=os.environ.get("HF_HOME", None),
    )
    model.eval()

    if args.limit is not None:
        data = data[:args.limit]

    use_sampling = args.temperature > 0
    all_results = []
    for idx, item in enumerate(tqdm(data, desc="Scoring")):
        # Build the full prompt by filling in document and summary
        document = item["source"]
        summary  = item["system_output"]
        prompt   = prompt_template.replace("{{Document}}", document)\
                                  .replace("{{Summary}}", summary)

        # Tokenize and move to device
        inputs = tokenizer(prompt, return_tensors="pt").to("cuda:0")
        input_length = inputs.input_ids.shape[-1]

        # Run generation and collect scores
        with torch.no_grad():
            generation = model.generate(
                **inputs,
                max_new_tokens=args.max_new_tokens,
                return_dict_in_generate=True,
                output_scores=True,
                top_k=args.top_k,
                do_sample=use_sampling,
                temperature=args.temperature if use_sampling else None,
            )

        # Extract generated sequence and per-step logits
        sequences = generation.sequences        # shape [1, input_length + new_tokens]
        scores    = generation.scores           # list of length new_tokens, each [1, vocab_size]

        # Decode only the newly generated tokens
        generated_ids = sequences[0, input_length:].tolist()
        generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)

        # Compute logprobs and top-k candidates at each step
        tokens        = []
        token_logprobs = []
        top_logprobs   = []
        for step_idx, step_scores in enumerate(scores):
            # step_scores: [1, vocab_size]
            log_probs = F.log_softmax(step_scores, dim=-1)  # shape [1, vocab_size]

            # actual token id & its logprob
            token_id = sequences[0, input_length + step_idx].unsqueeze(0)
            lp = log_probs.gather(1, token_id.unsqueeze(1)).item()

            # top-k tokens and their logprobs
            topk_lp, topk_ids = log_probs.topk(args.top_k, dim=-1)
            topk_lp   = topk_lp[0].tolist()
            topk_ids  = topk_ids[0].tolist()
            topk_dict = {
                tokenizer.convert_ids_to_tokens(tok): logp
                for tok, logp in zip(topk_ids, topk_lp)
            }

            tokens.append(tokenizer.convert_ids_to_tokens(token_id.item()))
            token_logprobs.append(lp)
            top_logprobs.append(topk_dict)

        # Format one “completion” record similar to OpenAI API
        completion_record = {
            "id":              f"localcmpl-{uuid.uuid4().hex[:8]}",
            "object":          "chat.completion",
            "created":         int(time.time()),
            "model":           model_name,
            "choices": [{
                "index":      0,
                "message":    {"role": "assistant", "content": generated_text},
                "logprobs": {
                    "tokens":         tokens,
                    "token_logprobs": token_logprobs,
                    "top_logprobs":   top_logprobs
                },
                "finish_reason": "length"
            }]
        }

        # Merge with original item
        item_output = dict(item)
        item_output["judge"]    = generated_text
        item_output["logprobs"] = completion_record["choices"][0]["logprobs"]
        all_results.append(item_output)
        
    # Write out final JSON
    with open(args.save_fp, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)

    print(f"\nScoring complete! Results saved to {args.save_fp}")

if __name__ == "__main__":
    main()

